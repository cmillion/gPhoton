% Uncomment below for AASTEX
\documentclass[preprint]{aastex}
% Uncomment below for EMULATEAPJ
%\documentclass[iop]{emulateapj}
\usepackage{url}
\usepackage{color}
\shorttitle{gPhoton}
\shortauthors{Chase Million et al.}
\usepackage{graphicx}
<<<<<<< Updated upstream
=======
\usepackage{epstopdf}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
>>>>>>> Stashed changes
\begin{document}
\title{gPhoton: A Time-Tagged Database of GALEX Photon Events}

\author{
Chase Million\altaffilmark{1},
Scott W. Fleming\altaffilmark{2,3},
Bernie Shiao\altaffilmark{2},
Mark Seibert\altaffilmark{4},
Randy Thompson\altaffilmark{2,3},
Myron Smith\altaffilmark{5},
Richard L. White\altaffilmark{2},
Karen Levay\altaffilmark{2,3},
Parke Loyd\altaffilmark{6}}

\email{chase.million@gmail.com}
\altaffiltext{1}{Million Concepts LLC, 2204 Mountain View Ave., State College, PA 16801, USA}
\altaffiltext{2}{Space Telescope Science Institute, 3700 San Martin Dr, Baltimore, MD 21218, USA}
\altaffiltext{3}{Computer Sciences Corporation, 3700 San Martin Dr, Baltimore, MD 21218, USA}
\altaffiltext{4}{The Observatories of the Carnegie Institution of Washington, 813 Santa Barbara Street, Pasadena, CA 91101, USA}
\altaffiltext{5}{{\color{red}NOAO...need full address from Myron.}}
\altaffiltext{6}{Department of Astrophysics and Planetary Science, University of Colorado, Boulder CO}

\begin{abstract}
We describe the gPhoton project, an effort to archive the complete corpus of approximately 1.1 trillion photon events recorded by the GALEX spacecraft. This effort includes the development of a pure Python, standalone calibration pipeline that reproduces core functionalities of the GALEX mission's original calibration pipeline, but also generates lists of aspect corrected photon level data. A suite of software tools simplifies use of the photon level data as hosted in a MAST database, with particular emphasis on short time domain analyses. The software is freely available as an open-source application\footnote{\url{https://github.com/cmillion/gPhoton}} and will continue to be updated in the future. We describe the set of software tools that are available, highlight differences between the gPhoton standalone calibration compared to the original mission's pipeline, and demonstrate the initial performance of gPhoton's calibration by analyzing standard star fluxes, measuring the pipeline's astrometric precision, and comparing these against the official mission archive. We highlight some of the science enabled by gPhoton, including stellar flares, high cadence sampling of known variables, and cross-mission analyses (exemplified through simultaneous GALEX observations of Kepler targets). The database is housed at the MAST archives, and extends the utility and impact of GALEX data in the absence of dedicated, large-scale, space-based, UV-survey missions in the near future.
\end{abstract}

\section{Introduction}
The Galaxy Evolution Explorer \citep[GALEX;][]{mar2005} was a NASA Small Explorer telescope that spent approximately ten years surveying the sky in the ultraviolet. GALEX had a 1.25 degree field-of-view in two ultraviolet (UV) bands, centered around $1528\,\rm{\AA}$ (‚ÄúFUV‚Äù) and $2271\,\rm{\AA}$ (‚ÄúNUV‚Äù). Each band used its own microchannel detector, and had both a direct imaging mode and a slitless spectroscopic mode via a $\rm{CaF_{2}}$ grism. The spacecraft was launched on 28 April 2003 and operated until 28 June 2013, although the FUV detector failed in May 2009. On 4 May 2010, an event occurred that is referred to as ``the Course Sun Point'' (CSP), which generated substantial streaking in the NUV detector's Y-direction. Although the effect is largely correctable through subsequent calibration and onboard adjustments, observations taken between 4 May and 23 June 2010 have reduced point spread functions. NASA support for the mission ended in February 2011. Spacecraft ownership was then transferred to the California Institute of Technology for the ‚ÄúComplete the All-sky UV Survey Extension‚Äù (CAUSE) phase of the mission, during which operating costs were solicited from individuals or institutions. The GALEX mission collected data over {\color{red}some number of eclipses}, observing {\color{red}some fraction of the sky}, and accumulating {\color{red}some amount of} approximate total raw data volume and {\color{red}some other amount of} approximate reduced data volume.

The spacecraft, detectors, and calibration are well described in \citet{mor2005,mor2007}. We will attempt to restrict discussion to elements that are not discussed in those papers, have changed in the intervening years, or apply only to the gPhoton project. In Section \ref{motivation} we describe the motivation behind constructing the gPhoton database and software suite. In Section \ref{database} we describe the design and content of the $\sim 1.1$ trillion row database hosted at the Mikulski Archive for Space Telescopes (MAST). In Section \ref{softwaretools} we describe the main programs available to use with the databse for constructing lightcurves, images, and animated data cubes.  In Section \ref{implementation} we highlight some of the implementation challenges we came across along with our solutions, some of which may be applicable to other photon event databases. In Section \ref{calibration} we present tests of the calibration precision by studying the astrometric, relative, and absolute fluxes produced by the software.  Finally, in Section \ref{scienceexamples}, we highlight a few example science cases enabled by gPhoton, including the study of stellar flares, high cadence sampling of known variables, and cross-mission science that can be conducted by combining gPhoton with data from other missions.

\section{Motivation}
\label{motivation}
Microchannel plates are non-integrating photon detectors. That is, rather than accumulating detector events in integrated bins, the microchannel plates record position and time information for every event individually. Due to computer storage and processing constraints, the GALEX mission team only released integrated maps of these data on per-observation timescales of hundreds to thousands of seconds. While the GALEX detectors were capable of recording events with a time resolution of five thousandths of a second, and a spatial resolution of {\color{red}something} on the detector, the high time and spatial resolution photon level data was not released to the community, except in isolated cases by special request, and no serious attempts were made to understand the detector performance or calibration parameters for integrations shorter than about 100 seconds. Furthermore, by the end of the mission, the GALEX calibration pipeline software had grown to sufficient complexity that no attempts to get it to run outside of the GALEX network of computers at Caltech were successful. With advances in storage capacity and processing capability, we have undertaken this project to build a standalone GALEX calibration pipeline, using it to generate and archive aspect-corrected positions for the approximately $\sim\;1.1$ trillion GALEX photon events accumulated over the course of the whole mission, and a suite of software tools to use those photons to construct lightcurves and images at user-specified spatial and temporal scales.  While gPhoton does reproduce the core functionality of the GALEX mission calibration pipeline in an open source and simple to use package, it is not intended to fully reproduce the mission pipeline. Rather, the intention is to create of a set of tools that serve as a starting point for researchers to perform their own scientific analyses on the reduced data, which might include refining the GALEX calibrations entirely.

The spacecraft observed during stretches of time when the Sun was eclipsed by the Earth, known as ``eclipses'', which typically lasted 1600-2000 seconds. To avoid detector burn-in or local gain sag effects caused by depletion of electrons in the multiplier plate, the GALEX spacecraft did not statically stare at locations on the sky, but continued to move over the course of each observation. Several observing patterns or ‚Äúmodes‚Äù were used over the course of the mission with potential consequences for the nature of the underlying data. The most basic mode was referred to as the ‚Äúdither pattern‚Äù in which the spacecraft boresight would trace out a tight spiral (or ‚Äúdither‚Äù) with a radius of a few arcminutes. The dither pattern was used most often for deep or medium imaging surveys in which each full eclipse of approximately 1600 seconds was spent observing a single region of the sky. In the All-sky Imaging Survey (AIS) observing mode, the spacecraft would jump between multiple positions (typically 12 ‚Äúlegs‚Äù) on the sky for short integrations of ~100 seconds each with the detector set to a non-observing state during the transition between each position, producing one independent GALEX observation ("visit") per position. The ‚Äúpetal pattern‚Äù mode, used to observe brighter targets, was similar to the AIS mode except that the positions (typically 8 legs) were tightly clustered within the approximate area of a single FOV and the detector continued to observe in the transition periods, spreading the light of any single source more widely across the detector. Finally, a ‚Äúscan mode‚Äù was implemented late in the mission--after detector count rate safety limits were relaxed, and used frequently during the CAUSE phase--in which the spacecraft boresight would rapidly traverse many degrees of sky at a time. Scan mode data presents unique challenges for analysis over the other modes, including uncertain calibration and frequent failure of aspect refinement; while gPhoton can use scan mode data, results in these cases should be used with additional care and caution.

\section{Description of the Database}
\label{database}
<<<<<<< Updated upstream
The construction of a trillion-row SQL Server database requires advanced planning in order to make queries more efficient.  The first step in generating the gPhoton database was to create the aspect-corrected photon events as comma-separated values (CSV) files.  Starting with the raw photon event (-raw6), refined spacecraft attitude (-asprta), and spacecraft state (-scst) files available at MAST, the celestial positions of the photon events are calculated and then exported to the CSV file.  At this stage, the CSV file contains the time of the photon event, event positions on the detector, the aspect-corrected positions on the sky of those events (RA,DEC), and status flags used to track a variety of conditions related to the detector readout.  Only a subset of the photon events from a given eclipse are able to have aspect-corrected positions calculated (most often because only a subset of photon events in an eclipse correspond to an observation, others happen during dead time, slewing, etc. {\color{red}I don't think this explanation is quite correct, Chase please revise.}).  Therefore two CSV files for each band are created: a CSV file for {FUV, NUV} data that contain aspect-corrected photon events, and a CSV file for {FUV, NUV} data containing those photon events that were not aspect-corrected.  The non-aspect-corrected photon events are used for estimating deadtime corrections, and so are uploaded into the database, but are otherwise not used beyond this point.

We divide the sky into ten different databases, based on declination.  These ten databases are then further sub-divided into a total of 999 partitions.  Each of these 999 partitons are composed of a number of ``zones'', which are stripes of declination $30''$ in height.  Photon events from the aspect-corrected CSV file for an eclipse are then assigned into one of these ten databases, taking into account that some eclipses may span more than one database/partition/zone.  In order to quickly assign a given photon event to the correct database, we make use of the zone match algorithm of \citet{gra2006}, both for loading and querying the database.  The distribution of the ten databases are shown in Fig.\ \ref{dbdist}, along with the DEC range of each.  The gPhoton software operates seamlessly across these database boundaries when queries span multiple databases, but most queries only need to operate within a single database, which helps improve performance.

Both the database boundaries and the number of $30''$ zones assigned to each parition were defined so that the databases are of similar sizes.  This was accomplished by assuming the total number of photon events in a given eclipse are distributed evenly across that eclipse's footprint.  Then, the cross section of that eclipse's footprint against the zones is calculated to determine which zones that eclipse overlaps.  The number of photons in each zone from this eclipse is estimated based on the cross-sectional area, e.g., if a given eclipse spans two zones, but only 10\% of the eclipse's footprint is in one of the zones, 90\% of its total photon events would be considered to belong to the first zone, and 10\% to the other.  This method of estimating photon events in each zone is not precise, since it assumes photon events are evenly distributed across the eclipse, but it does serve as a quick approximation to define the database boundaries and partition assignments, since it is not necessary to actually compute the zone membership for all the 1.1 trillion photon events beforehand.

\section{Description of the Software Tools}
\label{softwaretools}
There are four utilities functions included in gPhoton and described in {\color{red}Table 1}. These utilities are all written in Python and relased under a permissive license. With the expection of gPhoton, the tools can be called either from the command line or from within the Python interpreter prompt. If called from within the interpreter, output is returned in a Python data structure. These command line utilities draw upon a large a large number of supporting libraries which will not be described but are possibly of interest to researchers who wish to work more deeply with the GALEX data.
=======
\subsection{Description of Data Products from the Mission Pipeline Used By gPhoton}
During the GALEX mission, data were downlinked from the spacecraft and assembled into monolithic telemetry files (-tlm). The ingest stage of the mission pipeline split these into various types of encoded raw detector event and spacecraft state (-scst) data, which included coarse aspect solutions from the onboard star tracker at one-second resolution, as well as spacecraft housekeeping records. The most important class of encoded raw detector data for gPhoton, containing nominal scientific observations, are the -raw6 files. The -raw6 were decoded with a sequence of bitwise manipulations into lists of raw detector positions ($x$ and $y$) with timestamps for all detector events. These raw positions were further adjusted with ``static'' (detector-space) calibrations for wiggle, walk, nonlinearity and distortion, described more completely in \citet{mor2007}. For data collected after the Course Sun Point event (on approximately eclipse 37460), several onboard detector constants changed, propagating as changes to the calibration, and an additional processing step was added to correct for new streaking correlated with the ‚ÄúYA‚Äù value of the raw event telemetry. The ‚ÄúYA‚ value is an intermediate binary output of the bitwise manipulations required to convert raw telemetry into detector positions and time, and the physical meaning of the correlation between post-CSP streaking with this value is not fully understood, but the resulting correction, described in the technical documentation, permits reconstruction of the PSF in post-CSP data to within 0.1 arcseconds of pre-CSP data.

\subsection{Construction of the Database}
The database is populated using photon list files produced by running event extraction on the full corpus of data, up to and including GR7.  The first step in generating the database was to create aspect-corrected photon events as CSV files.  Starting with the raw photon event (-raw6), refined spacecraft attitude (-asprta), and spacecraft state (-scst) files available at MAST, the celestial coordinates of the photon events are calculated and then exported to the CSV file.  At this stage, the CSV file contains the time of the photon event, the event positions on the detector, the aspect-corrected positions on the sky (as RA and DEC), and status flags used to track a variety of conditions related to the detector readout. {\color{red}We should add these as a table if they might AT ALL be relevant.  We should err on the side of inclusion when it comes to details.}

Only a subset of the photon events from a given eclipse are able to have aspect-corrected positions calculated (most often because only a subset of photon events in an eclipse correspond to an observation, others happen during dead time, slewing, etc. Therefore, a total of four CSV files are created: two CSV files for FUV and NUV data that contain aspect-corrected photon events, and two CSV files for FUV and NUV data containing photon events that were not aspect-corrected. The non-aspect-corrected photon events are still loaded into a database, because they are used for estimating dead time corrections (see Section \ref{deadtimedesc} for details).

Null events can also arise from cases where events fall outside of the main detector FoV such as: erroneous data or stim events, {\color{red}events that are not aspect-correctable because they fall in a time period that is in a gap between refined aspect solutions [I don't understand this one, or how it's different from the next one (SWF)]}, events that occur between eclipses or visits {\color{red}[but why would these be outside the detector FoV?]}, or simply due to a failure of aspect refinement. The right ascension and declination values for such events are assigned a value of null in the photon list file. The distinction between this ``null data'' and nominal or ``non-null data'' is important, as it is treated differently in both the database and high level calibration. In the current implementation of the standalone calibration pipeline, events which fall on detector regions flagged by the mission team as containing hotspots are considered invalid and passed to the Null table without being aspect corrected. But because GALEX hotspots are known to be transient and, therefore, these regions may contain valid data, it is anticipated that a future revision to the calibration pipeline and database will aspect correct these events and move the hotspot flagging step to the client side (as with response flagging), giving end users the option of turning the mask on or off or redefining it completely.

For performance optimization purposes, the event-level data is distributed over multiple databases. The non-null (aspect corrected) data are spread across ten different databases organized by celestial declination, with bounding ranges selected to approximately balance the number of rows per database.  The ten database tables are further divided into a total of 999 partitions. Each partition is then further divided into ``zones'' of $30''$. We make use of the fast zone matching algorithm described in \citet{gra2006} for loading and querying the database.  Both the database boundaries and the number of $30''$ zones assigned to each partition were defined so that the databases are of similar sizes.  This was accomplished by assuming the total number of photon events in a given eclipse is distributed evenly across that eclipse's footprint.  Then, the cross-section of that eclipse's footprint against the zone boundaries is calculated to determine which zones that eclipse overlaps.  The number of photons in each zone from this eclipse is estimated based on the cross-sectional area, e.g., if a given eclipse spans two zones, but only 10\% of the eclipse's footprint is in one of the zones, 90\% of its total photon events would be considered to belong to the first zone, and 10\% to the other.

This method of estimating photon events in each zone is not precise, since it assumes photon events are evenly distributed across the eclipse, but it does serve as a quick approximation to define the database boundaries and partition assignments by not requiring to actually compute the zone membership for all 1.1 trillion photon events beforehand.  The distribution of the ten databases on the sky is shown in Fig.\ \ref{dbdist} {\color{red}Bernie: Please confirm boundaries are correct.}, along with a table summarizing the declination ranges and number of photon events in each database {\color{red}Scott will make this table once Bernie gets him the numbers.  Bernie, I would need number of non-null photons in each database for FUV and NUV (separately).  Also, if there are some events in the null table that we will be moving into the non-null table, it would be good to include thos in this count.}. The assignment to a database or partition is strict, leaving open the possibility that a user's query spans two databases. The majority of normal queries access only a single database, but queries that do span multiple databases are handled on the server, transparent to both the software and end users. Null data which were not aspect corrected reside in a single database.  {\color{red}How are the non-null and null tables indexed (which columns)?}

\begin{figure}
\includegraphics[scale=0.48]{FigDBDist}
\caption{The DEC boundaries of the ten databases that comprise gPhoton.  There are a total of 999 partitions across the ten databases, each with a variable number of $30''$ zones (stripes of DEC).  The number of zones in each partition, and the number of partitions in each database, were assigned so that the size of the ten databases would be roughly equal to each other. \label{dbdist}}
\end{figure}

\section{Description of the Software Tools}
\label{softwaretools}
Within gPhoton, in general, sky positions are reported as a two-element vector (right ascension and declination), both in J2000 decimal degrees. Time ranges (or ``bins'') are defined as two-element vectors where the first element is the start time and the second element is the end time. The gPhoton project defines timestamps in ``GALEX time'' throughout, where $t_{\rm{GALEX}} = t_{\rm{UNIX}} - 315964800$. Search ranges (in both space and time) are generally taken to be inclusive of the lower value and exclusive of the higher value, to eliminate duplicate counting of data at boundaries.

By default, the database tools define an effective detector FoV 1.1 degrees in diameter. This is to conservatively trim data observed on the edges of the GALEX MCPs, which suffer from poorly understood issues of sensitivity and spatial distortion. Data near the edges may be useful to cautious and knowledgeable investigators, though, so the effective detector size is adjustable from the command line. This conservative trimming of the effective FoV does not eliminate problems caused if there's a region of interest that extends towards the edge.  These can include photometric apertures, background annuli, or image footprints that extend past the edge of the effective FoV.  The best photometry will come from sources that are clear of both the physical and effective FoV boundaries.

There are four primary modules included in gPhoton and described in Table \ref{moduledesc}. These utilities are all written in Python and released under a permissive license. With the expectation of gPhotonPipe, the tools can be called either from the command line or imported as a module. If imported as a module, output is returned in a data structure. These command line utilities draw upon a large a large number of supporting libraries which will not be described in this paper, but are possibly of interest to users who want to perform advanced or specialized analyses with the gPhoton data.  For more information, users are encouraged to consult the documentation available in the software repository, or at the MAST page for the project: \url{https://archive.stsci.edu/prepds/gphoton/}.
>>>>>>> Stashed changes

\begin{tabular}{|p{2cm}|p{12cm}|}
\hline
	{\bf Utility} & {\bf Function}\\
	gPhoton & {Generates aspect corrected photon list files from a small set of user supplied input files which are nominally archived products of the mission pipeline. Output from gPhoton is used to populate the master photon event database hosted at MAST.}\\
	gFind & Provides information on the available data coverage for a given coordinate.\\
	gAperture & Creates lightcurves for a requested target (tables of times, calibrated fluxes, and additional parameters, with various calibrations applied).\\
	gMap & Creates calibrated count, response, or intensity maps (images) or data cubes (movies).\\
\hline
\end{tabular}

{\color{red}Table 1.}

\subsection{gPhoton}
The gPhoton standalone calibration pipeline implements only a minimum set of features from the full mission pipeline. The command line function, \textit{gPhoton.py}, accepts the raw scientific data file (raw6), the spacecraft state file (scst), and a refined aspect file (asprta), and returns a table in comma-separated-value (csv) format in which each row corresponds to a detector event and contains both raw and aspect-corrected detector positions, as well as sky positions and a number of intermediate detector parameters. The raw6, scst, and asprta files are products of the mission calibration pipeline that are archived at MAST {\color{red}Is it true that all of these are already archived?}. The raw6 and scst files are products of the ingest stage of the mission pipeline and simply contain parsed subsets of data that was originally in the raw spacecraft telemetry files (tlms). While the scst files contain many parameters related to spacecraft status, including the coarse estimated boresight pointing from the star tracker, only a single temperature value from the scst is used for the purpose of applying a temperature dependent plate scale correction. The asprta files, products of the aspect correction stage of the mission pipeline, contain refined boresight pointing estimates on a per-second basis, created by iteratively comparing 15-second depth intensity maps generated from the best-available aspect solution against star catalogs. By using these mission products directly, we do not need to recreate the ingest or aspect correction stages of the mission calibration pipeline.

In most cases, a flag column entries which is not equal to zero indicates that an event could not be aspect corrected for some reason. One exception is events which are flagged because they fall within a gap in the refined aspect solution during which the detector continue to observe (for example, between legs of a petal pattern observation); in such cases, the events are aspect corrected but flagged as having unreliable aspect solutions. Another exception is events which fall within regions of the detector known to be sources of hotspots which can be aspect corrected but should not be naively trusted. Events which are not aspect correctable for some reason (for example, those arising from the detector stims) are referred to as ‚Äúnull‚Äù data because they contain null values in the right ascension and declination columns of the photon list file. Null data can be useful for detector calibration and, in particular, exposure time correction based upon the stim or global count rates. Other than hotspots, the gPhoton project does not implement any artifact flagging or masking.

\subsection{gFind}
This utility provides a mechanism for the user to inspect the available data coverage for a particular target. Given a target position, it will return the estimated (raw) depth of available data over the whole mission, as well as the time ranges for specific observations in which these data are contained. Rather than using the visit-based bookkeeping of the mission logs to determine the available coverage of a specific location on the sky, \textit{gFind} attempts to use the data itself. A position on the sky is considered to be covered during time ranges in which it falls within half a detector diameter (an adjustable parameter with a default of 1.25 degrees) of the boresight center, as defined by the refined aspect solution (asprta). The contiguity of data for the purposes of grouping them into discrete time ranges is defined by a user adjustable parameter that defines the maximum gap allowed for data to be considered part of the same obvservation. There is a similar user-adjustable parameter that defines the minimum exposure depth considered to be a valid exposure.

\subsection{gAperture}
This utility performs photometric analysis given a target position, aperture radius, optional inner and outer radii of a background annulus, and optional time ranges. Simple aperture photometry is used, but the fact that the aspect corrected photon-level data is available allows us to perform a cone search on the data to precisely count the number of photons within the aperture rather than binning the photons into pixels--sacrificing spatial precision in the process--and then performing interpolations as would be done for normal integrated image data. The background counts are similarly computed. Rather than integrating a relative response map, the photon events can be individually weighted by using their detector positions to sample the flat directly. Effective exposure times (raw exposure time modified by effects of data dropouts and detector deadtime) are estimated from the count rates over the entire detector. The photometry can be computed within any time bin, up to and including an integration over the entire mission. The output format is a comma separated value format file containing a large number of columns which include raw counts, background rates, estimated errors, mean response over the aperture, and mean time of arrival for photon events within the time bin. When gPhoton is called from within the Python interpreter, the returned data structure contains a large number of additional parameters including the complete photon list data for the targeted region of the sky, permitting detailed analysis.

\subsection{gMap}
This utility generates integrated count, response, and intensity image maps for targeted regions and time ranges. All image data is written in the Flexible Image Transport System (FITS) with headers populated using World Coordinate System standards. Count (cnt) images are integrated images which have not been scaled by the detector response, generated by performing a box search for events in the requested region and then binning the events into pixels with the same spatial dimensions as products generated by the mission. High resolution relative response (rrhr) maps are created by adding successive projections of the detector flat onto the sky, using the boresight position, at one second intervals, scaled by effective exposure time. Intensity (int) maps are the "calibrated" image products from which photometric analyses may be performed, are simply count maps divided by relative response maps.

The images produced by gMap are analogous to the imaging data products produced by the mission pipeline and archived at MAST with the exceptions that users are not limited to images with specific dimensions and coadds can be created arbitrarily across visits and surveys. Also, each of these types of images can also be generated as data cubes or movie files with user specified time bins whereas full resolution and calibrated movie files were never available from the mission. Note that because the gnomonic projection used by GALEX increasingly distorts images near the poles, you can expect images generated by \textit{gMap} to, in general, not have the same aspect ratio in pixels as they have in degrees.

\section{Implementation Challenges and Solutions}
\label{implementation}
Numerous challenges have arisen during the development of this project, and our solutions to some of these may be helpful to other microchannel plate or photon level data investigations. The details may be of general interest to users of our software suite as well.

<<<<<<< Updated upstream
\subsection{Photon Level Analysis}
=======
\subsection{Exposure Dead Time}
\label{deadtimedesc}
Microchannel plates are subject to a global exposure ``dead time'' effect caused by the inability of the detector to process more than one event at a time. The effect therefore scales as a function of total global detector count rates: as count rates go up, the fraction of exposure lost to dead time likewise increases. The effect is linear as a function of ``global count rate'', up to approximately 109 counts per second in FUV and 311 counts per second in NUV, as described in \citet{mor2007}.  The global count rate is just the count rate across the entire FoV during an observation {\color{red} [Confirm this definition of global count rate.]}. Other MCP instruments have used this global count rate relationship as a means to estimate the dead time correction factor. The GALEX detectors, however, were each equipped with four built-in electrical pulsers (``stims'') that artificially generated events at four off-detector positions a commanded rate of 79 counts per second, summed between all positions. The mission pipeline extracted these stim events as a direct proxy for the exposure dead time effect. That is, the ratio of the measured stim count rate to the canonical stim count rate is equivalent to the ratio of the effective exposure time to the commanded exposure time. However, one must also consider the contribution of counting statistics in this estimate.
>>>>>>> Stashed changes

\subsection{Effective Exposure Time Calculation}
The exposure time correction and, in particular, the global dead time correction, was not reliable for short time ranges. This is because the dead time correction was computed in the original mission pipeline by comparing the measured stim rate against the nominal stim rate of 79 counts per second. At short time ranges (on the order of seconds), the variance in this rate due simply to Poissonian counting statistics made the error in the exposure time calculation sufficiently high as to make flux calculations meaningless. The solution was to use the empirical dead time correction, a linear fitted relationship of dead time as a function of the global detector count rate (which can be thousands of counts per second). Our tests have shown that this is a reliable estimator of the dead time correction and leads to stable/reliable effective exposure times at short time ranges.

\subsection{Background Correction}
In order to increase the reliability of the background correction, we‚Äôve implemented a ``swiss cheese'' algorithm that masks out known sources within the annulus.  The count rate within the effective area of the annulus following the ``swiss cheese'' masking is then scaled to the area of the aperture.

\subsection{Relative Response Correction}
The relative response (i.e. flat field) was not characterized at spatial resolutions that make it meaningful for short time domains. That is, because the detector is constantly in motion with respect to the sky during any observation, any given source samples a region of the detector. The resolution chosen for the flat field (approximately equal to the width of a single ``dither'') was chosen with the assumption that an integrated observation would smear out any sub-resolution detector response effects, and the uncertainties would average out. This assumption does not hold for short exposure times. We have no solution for this, and it continues to be a problem. but we know of efforts by other investigators to characterize the flat at higher spatial resolutions.  This is one area that may be improved with future versions of our software.

\subsection{Flux Uncertainties}
{\color{red}We are going to need to explain the current way the pipeline estimates uncertainty, some of the challenges with deriving an accurate uncertainty estimate, and perhaps some ideas for improvements in the future.}
{\color{red}Note: Include a plot of LDS 749B vs. effective exposure time, like Chase has made, and comment.}

\section{Calibration Tests}
\label{calibration}

\subsection{Astrometric Solutions}
<<<<<<< Updated upstream
{\color{red}I'm not sure what the best tests here are, other than to trace the photocenters of objects across time?}

\subsection{Relative Flux Precision}
{\color{red}Here is where we will take a bunch of plates and study the variability as a function of distance from plate center, working under the assumption that most non-extended sources should be constant within their uncertainties.}
=======
We compare the mean photon position within the aperture to the MCAT source position for {\color{red}XXXX} randomly selected MCAT targets. One source of uncertainty here is that the {\color{red}[``CoB'' was used here, is that ``center of brightness?'']} was calculated by the mission pipeline on a pixel-based map, whereas gPhoton uses the photons directly. Another possible source of error is that one should ideally fit a two-dimensional Gaussian bounded by an off-center circle to find the center of brightness.  {\color{red}Were the same sources selected from each band?  Did the sources sample different observing modes, times, sky coordinates?}  We find that nearly all sources lie within one GALEX resolution element ($\sim 6''$), and that \{{\color{red}XXXX, YYYY}\} lie within {\color{red}$XXXX''$} in the FUV and NUV bands, respectively (Fig.\ \ref{fuvnuvastrometry}).

\begin{figure}
\includegraphics[scale=0.35]{FigAngSepFUVNUV}
\caption{Angular separation between gPhoton and MCAT source positions for {\color{red}XXXX} MCAT sources in both bands. \label{fuvnuvastrometry}}
\end{figure}

\subsection{Relative Flux Precision}
As a test of the relative photometric precision from gPhoton, we plot the difference between the MCAT magnitude and gAperture magnitude against the MCAT magnitude for {\color{red}XXXX} randomly selected MCAT sources.  One of our goals is to be able to reproduce MCAT fluxes for most targets over visit- and coadd-timescales.  All gAperture measurements were done at the MCAT source positions and over time ranges as reported in the MCAT, using an aperture radius of $0.1'$ (equivalent to ``APER4'' in the MCAT). To ensure sufficient signal to noise, only observations with depths greater than 100 seconds were used. To prevent any particularly deep fields or exposures from dominating the result, only fields with a total (estimated raw) exposure time of less than 5000 seconds were used.

We plot these comparisons for the FUV and NUV bands in Figs.\ \ref{fuvrelphot} and \ref{nuvrelphot}, respectively.  We find that {\color{red}XXXX\%} of sources agree to within 0.1 magnitudes.  {\color{red}We need to discuss why there might be a difference between the two, and also why bright sources in the MCAT are fainter in gPhoton.}  The fluxes shown in the comparison plots make use of the aperture-method for background subtraction.  We note that comparisons using the MCAT-values-method for background subtraction look quite similar.

\begin{figure}
\includegraphics[scale=0.35]{FigRelPhotFUV}
\caption{Comparison between MCAT and gPhoton FUV fluxes for {\color{red}XXXX} randomly selected MCAT sources with exposure times $100 \leq t \leq 5000$. \label{fuvrelphot}}
\end{figure}

\begin{figure}
\includegraphics[scale=0.35]{FigRelPhotNUV}
\caption{Comparison between MCAT and gPhoton NUV fluxes for {\color{red}XXXX} randomly selected MCAT sources with exposure times $100 \leq t \leq 5000$. \label{nuvrelphot}}
\end{figure}

>>>>>>> Stashed changes

\subsection{Absolute Flux Precision}
We have made use of the same white dwarf standard stars that were used by the GALEX mission pipeline to help calibrate and test our absolute flux precision.  Specifically, we use the white dwarf stars LDS 749B {\color{red}What are the other ones?}.

<<<<<<< Updated upstream
{\color{red}Note: Include as many white dwarfs from Camarota \& Holberg 2014 as possible to compare their final fluxes and ours.}
=======
\begin{figure}
\includegraphics[scale=0.35]{FigAbsPhotLDS}
\caption{Comparison between MCAT and gPhoton fluxes for LDS 749B in the FUV (green) and NUV (red). \label{ldsabsphot}}
\end{figure}
>>>>>>> Stashed changes

\section{Example Science Applications}
\label{scienceexamples}

\subsection{Flare Stars}

<<<<<<< Updated upstream
\subsection{Cross-Mission Overlap}
=======
\begin{figure}
\includegraphics[scale=0.375]{FigCRDraFlares}
\caption{Flares detected on CR Draconis using gPhoton, across the lifetime of the mission.  When available, FUV light curves are plotted (in blue) along with the NUV curves (in black).  Fluxes have not been aperture corrected. \label{crdraflares}}
\end{figure}
>>>>>>> Stashed changes

\section{Conclusion}
The GALEX data set has already proven to be extremely productive and is likely to be one of the most influential ultraviolet astronomical surveys for the foreseeable future. The gPhoton project simplifies, and in some cases enables, analyses using this data that were previously difficult or impossible, especially those related to short time domain photometry. While the gPhoton project is an effort to calibrate and make available the GALEX photon level data specifically, some of the techniques described here can be applicable to other observational databases that make use of non-integrating detectors, particularly microchannel plates. The fact that spatial analyses can be performed by making direct cuts on the photon level data rather than artificially degrading the spatial components of the data by integrating and interpolating onto image maps offers potential advantages in terms of both the flexibility of the data archive and the computational cost of analysis. The corresponding data management and volume issues associated with storing and retrieving massive amounts of photon level data is non-trivial, but also entirely soluble with appropriate use of existing database and storage technology. The behavior of the GALEX detector during very short timespans (which correspond to very small spatial sampling of the detector) is not well characterized, and further work on improving the resolution of the detector response, as well as correctly propagating flux uncertainties, will be required to derive the maximum utility from the photon level data.

\acknowledgements
Place acknowledgements here.

\begin{thebibliography}{}
\bibitem[Gray et al. (2006)]{gra2006} Gray, J, Nieto-Santisteban, M.~A., \& Szalay, A.~S.\ 2006, \emph{Microsoft Technical Report}, MSR-TR-2006-52

\bibitem[Martin et al.(2005)]{mar2005} Martin, D.~C., Fanson, J., Schiminovich, D., et al.\ 2005, \apjl, 619, L1
\bibitem[Morrissey et al.(2005)]{mor2005} Morrissey, P., Schiminovich, D., Barlow, T.~A., et al.\ 2005, \apjl, 619, L7
\bibitem[Morrissey et al.(2007)]{mor2007} Morrissey, P., Conrow, T., Barlow, T.~A., et al.\ 2007, \apjs, 173, 682
\end{thebibliography}

%%%%%%%%%%%%%%%%%% tables here %%%%%%%%%%%%%%%%%%

<<<<<<< Updated upstream
\clearpage
=======
\begin{figure}
\includegraphics[scale=0.375]{FigCRDraCoadd}
\caption{Deep coadd image of CR Draconis using all available NUV photon events.  The image is in counts, since we are only looking to define our photometric aperture and search for possible contamination sources.  The aperture, inner annulus, and outer annulus for photometry are represented by the green, orange, and red circles, respectively.  There is a source to the lower left that is $\sim 0.5$\% the peak of CR Draconis itself.  Although testing showed it did not have a major impact on the photometry significantly, we still define our apertures such that it is not included within them.\label{crdracoadd}}
\end{figure}
>>>>>>> Stashed changes

%%%%%%%%%%%%%%%%%% end tables %%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%% figures here %%%%%%%%%%%%%%%%%%
\clearpage

\begin{figure}
\plotone{FigDBDist.eps}
\caption{The DEC boundaries of the ten databases that comprise gPhoton.  There are a total of 999 partitions across the ten databases, each with a variable number of $30''$ zones (stripes of DEC).  The number of zones in each partition, and the number of partitions in each database, were assigned so that the size of the ten databases would be roughly equal to each other. \label{dbdist}}
\end{figure}
%%%%%%%%%%%%%%%%%% end figures %%%%%%%%%%%%%%%%%%

\end{document}
