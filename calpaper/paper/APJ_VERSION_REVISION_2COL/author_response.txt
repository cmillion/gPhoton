--------------------------------------------------------------------------------
RESPONSES TO REFEREE COMMENTS
--------------------------------------------------------------------------------
Fig 2 + 3: What are the blue lines? Gauss fits? Smoothed data?
--------------------------------------------------------------------------------
AUTHOR RESPONSE: The blue _curves_ in Figures 2, 3, 4, 5, 8 and 9 are Gaussian Kernel Density Estimates per these few sentences in the third paragraph of section 5: "In figures that include Gaussian Kernel Density Estimates (KDE), bandwidths were chosen by brute force cross-validation, and are represented by blue curves. The peak (“average”) of the KDE is reported as the peak of the distribution of data." For clarity, we have added a note to this effect in the captions to all of those figures. We made a small additional wording change to the caption of Fig. 9 for flow.

--------------------------------------------------------------------------------
5.3 Please read over this section again to check if some re-wording can make is easier to read. I think it's all correct, but it's not easy to follow, e.g. the last sentence "The modeled 1 sigma errors ... use these typical background values". If you have typical measured values, what needs to be modeled? After reading it again, I concluded that you probably model source + bg errors, based on these bg errors. Unfortunately, I don't have a simple suggestion how to fix this, but please look over this sentence and others in this section again.
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We removed these modeled errors from the plot in favor of curves that delineate the median MCAT error as a function of magnitude. See below.

--------------------------------------------------------------------------------
Fig 4 + 5: left panels: It looks as if the blue lines (errors) are all much larger than the actual spread in value? Is that so? Or is it just that you overplot a large number of error bars and I see only the largest values because they are all stacked? In this case, you might want to experiment with the opacity of the blue lines so that I can see that the error bars are denser in the middle or maybe only plot error bars for a random subsample of the values.
Caption: What are the red and blue lines in the right panels?
--------------------------------------------------------------------------------
AUTHOR RESPONSE: Yes. The (lower limits on) the errors are larger than the spread in values and also consistent with our modeled errors. The first point that we want to make is that gAperture does a good job of reproducing the mission photometry, i.e. within the measurement error. The second point we want to make is that this is even true for the systematic "swoop" in NUV due to differences in background estimation (top panel of Figure 5) even though it looks dramatic compared to the other plots. We struggled with how to visually depict this; the alpha value on the blue error bars was already set to a meager 0.025. We have decided to remove the error bars on the individual points and replace the curves delineating model errors with curves delineating the median 1-sigma errors as reported by the MCAT in 1 magnitude bins. This (1) is just as informative as the modeled error, (2) makes for a less confused plot, (3) allows us to avoid the awkward explanation of how the model works that was noted above. We have also added legends to the left panels of Figures 4 and 5.

The red lines in the right panels are the median value for the whole sample, which is the value reported in the legend. The blue curves are KDE estimates and a note has been added to the caption about these, per the earlier comment.

--------------------------------------------------------------------------------
Fig 9 caption: evidence -> evident
--------------------------------------------------------------------------------
AUTHOR RESPONSE: Fixed.

--------------------------------------------------------------------------------
Fig 11: I would remove the "30 sec" line from the legends. It's fairly obvious what that lines means from the x-axis and it's very different from all the other lines labelled in the legend.
--------------------------------------------------------------------------------
AUTHOR RESPONSE: Removed.

--------------------------------------------------------------------------------
RESPONSES TO EDITOR COMMENTS
--------------------------------------------------------------------------------
Very minor comment - in Figure 1, DEC should not be written like an acronym but I would just spell out declination instead (since there is space)
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We made this modification in Figure 1. We also took the opportunity to make more consistent use of "right ascension and declination" throughout the text.

NOTE:
Since Dr. Günther has chosen to de-anonymize himself, we have thanked him by name in the acknowledgements. We'd be happy to keep the acknowledgement anonymous if he prefers.

--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
RESPONSES TO FIRST ROUND OF EDITS
--------------------------------------------------------------------------------
We thank the referee for the thorough review of our paper and software. In addition to addressing the referee's comments, we have made some additional code changes since the paper was initially submitted (which was v1.27.1), and the most recent version (v1.27.2). These code changes directly produced no change to the calibration results presented in the paper at the precision used. However, we've also taken the opportunity to make the sample size more consistent between the two bands, which precipitated very minor changes to the final results (~1% in some places), and have refined our modeled error estimates to take into account the effect of the aperture, which necessarily produced some changes in the results of the LDS749B analysis.

Significantly, a comment by the referee led us to discover that the comparison of background methods was incorrect due to a unit conversion error and that our interpretation was therefore also in error. The section related to background has been substantially revised. Other parts of the analysis were unaffected by this error. (That is, the background was correctly removed from aperture photometry.) However, the new (correct) result also propagated to some changes in our interpretation of the relative photometry results.

We address each of the referee's points below. The compiled PDF has made use of the /edit1 macro in aastex6 to call out changes from the previous version. A summary of substantive changes made to the software to version 1.27.2 is provided at the bottom of this reply, though most are also tracked in GitHub as Issues under the "1.27.2" Milestone.

--------------------------------------------------------------------------------
RESPONSES TO EDITOR COMMENTS
--------------------------------------------------------------------------------
In addition to these comments, I just wanted to mention that while it's great that you have provided scripts to reproduce the plots in the paper, I would like to ask you to avoid including a Python pickle file, since these have no guarantee of working in the long term. Please use archival formats instead, whether plain text, JSON, FITS, HDF5, or other similar formats.
--------------------------------------------------------------------------------
AUTHOR RESPONSE: The data that were previously contained in the pickle files have been added as columns of other CSV files that shared the same row definitions. The figures.py scripts have been updated to reflect this change.
--------------------------------------------------------------------------------


--------------------------------------------------------------------------------
RESPONSES TO REFEREE COMMENTS
--------------------------------------------------------------------------------

Abstract
--------
"photon event" - This term may need a specific definition (are there other event types? Are there photons that are not events?). For the abstract I suggest to use the simple term "photon", leaving the exact definition for the text.
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We have simplified the abstract to refer to "photons" instead of "photon events," and in the Motivation section we describe more precisely what we mean by "events" in general, as distinct from "photons" or "photon events."
--------------------------------------------------------------------------------

Which version of the software does this describe? I understand that software evolves and articles do not, but the tests described here are done with one specific version e.g. when the authors say "summarize photometric and astrometric performance ...", those metrics might be version dependent.
--------------------------------------------------------------------------------
AUTHOR RESPONSE: The revised paper refers to v1.27.2 of the code, which has been merged to master and is available on PyPI at the time that we are submitting these revisions. The code version is recorded in the __init__.py, which is propagated to pip. We have added the version that this paper describes in the last paragraph of Section 1, and the last sentence of the Abstract.
--------------------------------------------------------------------------------

The word "python" turns up a couple of times. In some places it is capitalized, in others it is not. According to the PSF (Python software foundation) "the name of our favourite programming language is always capitalized" (https://pl.python.org/docs/doc/style-guide.html).
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We have standardized capitalization of Python throughout.
--------------------------------------------------------------------------------


Sect 1
-------
I'm missing a few more introductory sentences about "photon events".
This is common in x-ray and gammy-ray astronomy, but even UV astronmers traditionally think about integrated quantities, fluxes etc.
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We have added sentences in the second paragraph of Section 1 describing what the MCPs record, and how those data are reduced (to RA, DEC, time stamps, and fluxes) at a very high level. This is in addition to the description in Section 2 where we talk a bit more about what causes events (i.e. not necessarily astrophysical photons).
--------------------------------------------------------------------------------


Sect 4
-------
Where individual tools are described, the reader would profit from a simple example for each of them that shows an examplary call on the command line or in the python interpreter (the documentation of all parameters an be left for the user manual).
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We have added very basic examples of how to run three of the four main commands in Table 2 to their descriptions within that table. The fourth command -- gPipeline, which we note will be of minimal interest to most users -- requires additional, local data products to work properly; a description of what those are, how to obtain them, and how to run gPipeline is provided in the User Guide, so the table directs readers to that resource. The section on the example case of CR Draconis now also contains example commands.
--------------------------------------------------------------------------------

The gPhoton project defines timestamps in units of "GALEX time" throughout, where GALEX time is defined as a linear offset from UNIX or POSIX time, where tGALEX = tUNIX − 315964800 seconds.
However, UNIX time does not handle leap seconds properly. Two leap seconds occured during the GALEX mission. How is this dealt with?
--------------------------------------------------------------------------------
AUTHOR RESPONSE: Like UNIX time on which it is based, GALEX time does not handle leap seconds. Users requiring this level of precision will need to apply a correction. We've added a note on this issue to the User Guide. As a convenience, we do include a few time conversion methods in "gphoton_utils", which take GALEX time as input and returns JD_TDB, JD_UTC, JD_TAI, or a calendar date (in UTC). These methods make use of the astropy.Time module, so the precision in converting between time standards and accounting for leap seconds is limited by the precision inherit to AstroPy's implementations. gPhoton also does not currently adjust times to barycenter, which can be a much larger effect than two leap seconds; we plan to add barycentric correction in the future, but another note has been added to the User Guide in the meantime.
--------------------------------------------------------------------------------

The gPhoton software is described in several places as a dabase of individual photons. However, after reading this manuscript, the User Guide, and installing and playing around with the software, it is still unclear to me which steps allow the user to retrieve a full calibrated photon list. (gAperture only retunred binned quantities. gPipeline is supposed to return a photon list, but unlike the other tools it requires a raw6 and scst file as input. Where would I get those? Why can they not be downloaded automatically with the skypos and time as in gAperture?)
--------------------------------------------------------------------------------
AUTHOR RESPONSE: The photon (events) are available under the 'photons' key in the dictionary data structure that is returned by gAperture when run as a module. For example, if you set the return of gAperture to a variable: `x = gAperture(...)`, the events would be stored in `x['photons']`. Prompted by this note, we have also added a gAperture option (in v1.27.2) to save the unbinned photon events as a CSV file (with, e.g., `photoncsvfile=<...>`) when called from _either_ the command line or as a module.
--------------------------------------------------------------------------------

gPipeline: Do photon list files contain the values for the weighting factors for flat and dead time correction that are applied by gMap or gAperture tools? In other words, do the photon lists contain all the informnation needed for the analysis in some form or are the later tools required to download/calculate more information?
The authors put an emphasis on the fact that single-photon retrievel is possible, so this point should be mentioned.
--------------------------------------------------------------------------------
AUTHOR RESPONSE: The output of gPipeline (the photon list files) do not contain calibration values such as this. However, the photon lists returned by gAperture contain all information needed, including the flat values. We have added more description in the document describing the single-photon retrieval capabilities.
--------------------------------------------------------------------------------

In the high-energy community (X-ray, gammay ray), astronomers work almost exclusivley with single event data and methods and software packages exisit for e.g. source detection, or lightcurve analysis (e.g. Baysian block algorithm) that do not bin events, but treat individual events to calcualte likelyhoods. Those users would profit from having as much information as possible availalbe without the need to consult tools that bin the data into images or lightcurves.
--------------------------------------------------------------------------------
AUTHOR RESPONSE: When used as an imported module, the data structure returned by gAperture includes the unbinned data. We have also added the option to save photon events to a CSV file when running in command-line mode as well (via the "photoncsvfile" option). We have also updated the UserGuide entry in gAperture to emphasize these features.
--------------------------------------------------------------------------------

Sect 5
-------
In Sect 5.2 the authors suggest that the difference in the background they see compared to the MCAT is due to sources in the background annulus and they suggest that software users manually identify contaminating sources in the images. They should try out this suggestion for a few cases and report the differene between the MCAT background and that measured from an annulus placed by hand to avoid contaminating sources, since this is what they recommend users to do.
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We thank the referee for this comment which revealed an error on our part. We had not properly converted one of the values to the correct units of delta-Magnitude / arcsec**2, which has been corrected. To aid with interpretation as compared to the other analyses, we now report in units of effective magnitude within the aperture (i.e. surface flux multiplied by the area of the aperture). This has resulted in substantial revisions to Section 5.2. Per the initial comment, we have visually verified that a very large fraction of observations in the long tails of each band in Figure 3 have relatively bright nearby stars.

An upshot is that we realized that the observed effect in NUV photometry using the annulus method, although dramatic on the scale of the graphic, is within measurement error for stars at these magnitudes. There is clearly a systematic arising from some difference in the two background measurement methods, but it is not significant at the level of an individual measurement. We have modified the figures and text in Section 5.3 to illustrate this.
--------------------------------------------------------------------------------

In Fig 6, there seems to be a systematic trend, such that all exposures > 150 s produce values above the expected magnitude. Is there a known reason?
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We do not know of a reason, nor did anyone on the mission team who we asked. It doesn't appear to be an issue for "normal" (non-calibration survey) data. We actually consider the fact that gPhoton reproduces this apparent systematic to be evidence of our successful reproduction of the mission calibration pipeline at the visit level: because this trend is present in the mission-produced MCAT itself, and reproduced in a "blind" sense by our software, even though we hadn't granted much attention to the trend until it was pointed out here.
--------------------------------------------------------------------------------

The last few sentences of Sect 5.4 suggest to filter on leg. Are there instructions for users how this can be done?
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We have added a description on how to filter on leg in the UserGuide.  We would prefer not to put that description in the paper because the exact method may change. (The current method is rather slow; we have a plan for making it very fast, but it will require generating a new table in the database. The condition of "FUV + CAI + legs 1,2,3" will also get a flag value in the light curve files, until such time as we might fix the issue completely.) We added a note to Section 5.4, to consult the User Guide for how to filter on leg.
--------------------------------------------------------------------------------

Sect 6
-------
6.3 Relative response correction: The authors describe that the original version of the pipeline used several interpolations to calcualte the appropriate response for each pixel on the detector. In contrast, the new software avoids that.
One common reason for interpolating response maps is that the original flat-field data is noisy and has to be smoothed by interpolation. Did the authors verify that that is not a problem in this case?
--------------------------------------------------------------------------------
AUTHOR RESPONSE: Flat-field noise can be a problem, but is generally restricted to the detector edges where the response is less well characterized. Indeed, this is one of several reasons we caution the user to avoid the edges (and by default, many of the gPhoton routines to do so automatically). Short time-domain light curves of sources that stay within the linear response regime of the detector and near the center of the field-of-view have not shown strong correlations with detector position in any of our analyses.

Since the initial submission, we've discovered that targets with brightnesses in non-linear regime of the detectors often present with a false variability on intra-visit timescales that is strongly correlated with detector position. It is the single most common cause of false positives among many thousands of real science targets analyzed so far. We have added a note about this as section 6.5.
--------------------------------------------------------------------------------

Sect 6.4 hotpsots: The authors acknowledge that overlooking that a source moved through a hot spot can lead to spourios time variability. I strongly encourge them to ensure that their software alerts a user to this condition e.g. by means of a warning or by returning nan values in the lightcurves or images.
--------------------------------------------------------------------------------
AUTHOR RESPONSE: Version 1.27.2 flags for hotspots quite conservatively, so following the general advice to avoid any binned fluxes with non-zero flags will greatly reduce this problem. Our team has analyzed short time domain light curves of several thousands sources for significant variability since this change and found no instances of unflagged false positives due to hotspot masking. (Whereas we had found quite a few previously.) Future software versions will attempt to improve the hotspot flagging to avoid rejecting too many valid events. Yet still, our experience has taught us that one should _never_ trust the light curve without visually inspecting the corresponding gMap movie images. We have strengthened the language in the hotspot section to emphasize this point.
--------------------------------------------------------------------------------

Sect 7
------
Since this is meant as a usage example, I suggest to give the main commands that where used verbatim. There is no need to show all the plot code (I agree with the authors that makes a good attachement as a separate file), but as a reader I probably want to see an example of how to call the relevant modules at this point.
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We have added a paragraph showing how we made use of gFind, gMap, and gAperture to create the CR Draconis light curves (2nd paragraph in Section 7 now). In that paragraph we include the calls to gFind, gMap, and gAperture (examples are used when imported as modules, command-line would be almost the same).
--------------------------------------------------------------------------------

Fig 12, caption:
"such points can be identified by a flag" - I suggest giving the name of the flag here.
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We've added the specific bit to check in the flag bitmask to the figure caption (bit 4 is the one for nonlinearity warning). The bitmask look-up table is also in the User Guide.
--------------------------------------------------------------------------------

Fig 13:
I understand that this analysis is merely a usage example, but it should be correct and well readable.
In this respect Fig 13 is lacking. What are the coordiantes on the x and y axis? What is the color scaling? Why is the background annulus (i) so close to CR Dra that is is visibly dominated by the wings of CR Dra's PSF? Why is it so small (since there are fewer counts in the background than in the source, I would expect that you chose a large background region. Otherwise the error on the CR Dra flux will be dominated by the counting statistics in the small background aperture)?
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We have re-made Fig. 13 in several ways.  First, we show not the coadd (which includes data across all observations, and is not actually relevant for aperture definition) but instead show a 10-second frame from the brightest flare event.  We have included a coordinate grid (for spatial reference) and color bar (for intensity reference), and have further described in the figure caption the aperture sizes and choice of intensity scaling.  We have also remade the light curve plot itself using this refined aperture selection, which resulted in essentially the same plot.
--------------------------------------------------------------------------------

Figures
-------
There is no need to group figures and tables at the end of the manuscript any longer. ApJ/AJ used to have such a policy, but ended it many years ago and at least this referee finds it preferrable to place them in the text close to the position where they are discussed.
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We've moved the figures closer to their references in the text and created a 2-column version with aastex6, which is what is being submitted. The 2-column version also revealed the need to increase font sizes on many of the graphics for readability, which we've done.
--------------------------------------------------------------------------------

Figures look a little blurred. I can referee them fine, but I encourange the authors to make use of a vector graphics format such as pdf for most of their plots.
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We were unable to reproduce this issue, but we now use PDF versions of the figures instead of EPS.
--------------------------------------------------------------------------------


Software on github
===================

Installation
------------
Manually modifying the PYTHONPATH is not the best practice solution because it might interfere with e.g. virtualenv, conda environments etc. The de-facto standard is using setuptools, where the names of relevant command line scripts are defined as "entry points".
--------------------------------------------------------------------------------
AUTHOR RESPONSE: This was necessary early on when installing from GitHub and *not* running setup.py. Since this is not how most users will ever want to install gPhoton, we have removed mentions of modifying PYTHONPATH and instruct users to either use pip or clone from GitHub and execute the setup.py file directly.
--------------------------------------------------------------------------------

Installations instructions Ubuntu: Note that you can get astropy through "apt-get" as well.
--------------------------------------------------------------------------------
AUTHOR RESPONSE: Noted, and a comment was added to the User Guide.
--------------------------------------------------------------------------------

When trying out the software I followed the instruction in docs/UserGuide.md and that did not work. pip installing into Anaconda will not activate the command line scripts (see note about "entry points" above). I strongly encourage the authors to fix this - incomplete installation instructions are probably the fasted way to turn a user to look elsewhere.
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We were unfortunately not able to reproduce this issue with the information provided. In a normal bug report situation, we would ask for additional information, but the anonymous nature of the review makes that difficult.  If the referee would like to send a follow-up (anonymously) through the referee channels, with more specifics on their python environment and perhaps a command log (commands issued while trying to install and the errors returned) we'd be happy to investigate this further. We have tested the latest branch using both pip and pip through Anaconda and could not get an error, so it's also possible the issue has been fixed in the latest branch as well, perhaps the referee could attempt this installation again.

One thing that we know is often confusing is that the call to pip _requires_ the `-i` argument:
    pip install -i https://pypi.python.org/pypi gPhoton

We also had one other user report a similar-sounding issue recently, and the root cause in that case turned out to be that Anaconda was semi-invisibly using Python 3+. (See below.)
--------------------------------------------------------------------------------

Calling from within the Python interpreter
------------------------------------------
This is a little more cumbersome than it has to be.
This is the current interface:

>>> import gPhoton
>>> import gPhoton.gFind
>>> gPhoton.gFind(skypos=[123, 23])
>>> import gPhoton.gAperture
>>> gPhoton.gAperture.gAperture(skypos=[123, 34])

would the following not feel more natural?

>>> import gPhoton
>>> gPhoton.gfind(skypos=[123,34])
>>> ...

this can easyly be done by simple edits of __init__.py
to include lines like:
from gFind import gFine as gfind
from gAperture import gAperture as gaperture
...
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We have decided to take this opportunity to clean up this behavior, although we've retained the intercap (because we like it, and it matches the command-line syntax). Imports now work as follows, e.g.:

        import gPhoton
        gPhoton.gFind(skypos=[10,20])
--------------------------------------------------------------------------------


Python version
--------------
The code is obviously written for python 2 and will fail on Python 3. It's probably not too hard to convert using 2to3 or six, but it should state somewhere which Python versions are supported and which versions of numpy, scipy, astropy, ... are tested and known to work.
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We plan to make gPhoton compatible with Python 3.x in addition to Python 2.7.x in the near future. For now, the software has only been tested on Python 2.7.x. We have added a note about this in the User Guide.
--------------------------------------------------------------------------------


Some comments on the code
=========================
I do not think that it is on the referee of the manuscript to also provide a complete code review for gPhoton. On the other hand, this paper is "just" the published reference that the autors presumably intend to be the standard reference to using that code; there is not a whole lot of science to review in the paper.
Thus, I did browse through some of the modules on github (commit 6c83159) to gain an impression if, as a science user, I would trust this code of if it's design raises any alarming flags with me.
(I have to admit, that I was tempted to open PRs for some of the issues, but that would clearly give away my name - and I generally leave it up to the editor to decide if the referee should remain anonymous.)

The points listed below are to a certain degree just issues of style, thus my comments should be treated a suggestions, not as "the referee required us to make this change".

Use of Python features
----------------------
While things work the way they are written, the authors could have made use of some more build-in python capabilities, for example:

gPhoton/gPipeline.py
Most of what function check_args does is to check is arguments are present - this can be enforced by using the "required" keyword argument in "parser.add_argument" or to convert stuff to strings (again, the argparser can do that already).
--------------------------------------------------------------------------------
AUTHOR RESPONSE: As far as we understand, the argparse module only gets used when the tools are called from the command line. check_args() is programmed in this way to enforce some minimal requirements when the tools are imported as modules, since we want to enable both use cases.
--------------------------------------------------------------------------------

Readability counts
------------------
In some cases, the code could be simplified (but all the following code is untested):

gPhoton/PhotonPipe.py - line 301-316 can probably be reduced to 4 lines of code:

cut_fptrx = np.array(fptrx[ix], dtype='int64')
cut_fptry = np.array(fptry[ix], dtype='int64')

cut[ix] = (np.any(walk_x[q[ix], cut_fptry: cut_fptry + 2, cut_fptrx: cut_fptrx + 2] != -999) |
np.any(walk_y[q[ix], cut_fptry: cut_fptry + 2, cut_fptrx: cut_fptrx + 2] != -999))

(a similar desing pattern is seen in line 276 of the same file and the idea to define
cut_fptrx = np.array(fptrx[ix], dtype='int64')
cut_fptry = np.array(fptry[ix], dtype='int64')
so that this does not have to be repeated in every line applies to other code blocks as well, e.g. line 380).
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We agree that this would be a clarifying simplification, but don't want to change this particular module at this time because it is the heart of the software that produced the values now recorded in the database (gPipeline). We're planning an upcoming modification to the database (to restore data covered by the hotspot mask), which will involve changes to this module and extensive retesting; we will make clarity revisions of PhotonPipe.py at that time.
--------------------------------------------------------------------------------

Docs
----
The authors have written all docstrings in a format that sphinx will read. Are the rendered API docs available somewhere (e.g. readthedocs or on github pages)?
--------------------------------------------------------------------------------
AUTHOR RESPONSE: We have added API documentation using sphinx autodoc and hosted on readthedocs.  A link to the readthedocs API is now available on the main page in GitHub.
--------------------------------------------------------------------------------


--------------------------------------------------------------------------------
SUMMARY OF CHANGES BETWEEN SOFTWARE BRANCH v1.27.1 (on submission) and v1.27.2.
--------------------------------------------------------------------------------

We have rerun our full suite of regression tests (including regenerating all graphics), which include the astrometry, relative photometry, and absolute photometry presented in the paper, and have found minimal difference between the two branches in terms of the results presented in the paper. Changes include:

1.) We have slightly modified the method of computing the "MCAT" background for a target. It still does exactly what is described in the paper, but it should do a better job of selecting the closest MCAT visit (in time and space) from which to draw a background estimate. The text (and User Guide) has been modified to describe the new feature, which happens in rare cases where no appropriate visit is found.  In those cases, the MCAT estimated background is set to NaN. Note that this did not, by itself, produce a change in results at the level of precision that they are reported in the paper. This is not unexpected because the change should really only effect cases where there is no nearby MCAT source; the calibration tests uses only locations of known MCAT sources and so avoid this case by construction.

2.) We took the opportunity of rerunning the calibration to make the baseline samples 10,000 targets in *both* bands to better permit direct comparison (10,000 targets is less than used in the original draft in NUV, and more in FUV). We have modified the description in the manuscript (and figures) accordingly. The calibration results changed only minimally due to this at the level of precision that they are reported in the paper.

3.) For consistency, all analyses use only data with _no_ gAperture flags unless otherwise noted. This produced some minor changes in results.

4.) The modeled errors in the analysis of LDS749B have been modified slightly to take into account the effect of counting statistics of the aperture correction. This necessarily resulted in a change in the reported % of data falling within 3-sigma.

5.) We added "lower limit" error bars to the data points in the analysis of relative photometry.

Additional changes:
+ Section 3.2 (describing mission pipeline data products) has been slightly expanded for clarity. We've made minor grammar and clarity edits throughout.
+ We have tightened up the figures and increased font sizes in a few places without altering the content in ways not already mentioned above.

For a complete history of changes since the initial paper submission, please see the list of Github issues under the v1.27.2 milestone and commit log starting from 6/2/2016 through the merge of branch v1.27.2 into Master.
